#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import time
import psutil
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import requests
import time as timer
from pathlib import Path

# ==========================================
# 1. ç¯å¢ƒé…ç½®
# ==========================================
SPARK_HOME = Path(os.environ.get("SPARK_HOME", "/usr/local/spark"))
SPARK_MASTER = "spark://172.23.166.133:7077"  # é›†ç¾¤Masteråœ°å€
SPARK_UI_PORT = 4040  # Spark UIç«¯å£
PYTHON_PATH  = "/usr/bin/python3"  # ç»Ÿä¸€Python 3.7è·¯å¾„
DRIVER_HOST = "172.23.166.133"

# é…ç½®Sparkç¯å¢ƒ
if SPARK_HOME.exists():
    os.environ.setdefault("SPARK_HOME", str(SPARK_HOME))
    os.environ["PYSPARK_PYTHON"] = PYTHON_PATH
    os.environ["PYSPARK_DRIVER_PYTHON"] = PYTHON_PATH
    
    spark_python = SPARK_HOME / "python"
    py4j_zip = next((spark_python / "lib").glob("py4j-*-src.zip"), None)
    
    sys.path.insert(0, str(spark_python))
    if py4j_zip and py4j_zip.exists():
        sys.path.insert(0, str(py4j_zip))
    
    print(f"âœ… Sparkç¯å¢ƒé…ç½®å®Œæˆ: {SPARK_HOME}")
else:
    print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°Sparkç›®å½• {SPARK_HOME}")
    sys.exit(1)

from pyspark.sql import SparkSession
from pyspark import SparkConf

class BaselineMatrixExperiment:
    def __init__(self):
        self.results_dir = "baseline_results"
        os.makedirs(self.results_dir, exist_ok=True)

        # é›†ç¾¤é…ç½®ä¿¡æ¯
        self.cluster_config = {
            "master": SPARK_MASTER,
            "driver_host": DRIVER_HOST,
            "python_path": PYTHON_PATH,
            "workers": 3,  
            "cores_per_worker": 1,  # æ¯ä¸ªworkeræœ‰1ä¸ªcore
            "memory_per_worker": "4.0 GiB",  # æ¯ä¸ªworkeræœ‰4GBå†…å­˜
            "total_cores": 3,  # æ€»æ ¸å¿ƒæ•° = 3 workers * 1 core
            "total_memory": "12.0 GiB"  # æ€»å†…å­˜ = 3 workers * 4GB
        }

        # 7ç»„å®éªŒé…ç½®
        self.experiments = [
            # 1000x1000
            {"n": 1000, "density": 0.1},
            {"n": 1000, "density": 0.01},
            {"n": 1000, "density": 1.0},

            # 3000x3000
            {"n": 3000, "density": 0.1},
            {"n": 3000, "density": 0.01},

            # 5000x5000
            {"n": 5000, "density": 0.1},
            {"n": 5000, "density": 0.01},
        ]

    def get_resource_config(self):

        return {
            "executor_memory": "2g",
            "executor_cores": "1",
            "driver_memory": "2g",
            "num_executors": "3",
            "partitions": 12,
        }


    def get_spark_stage_count(self, app_id, spark_master_url="http://172.23.166.133:4040", retries=5, delay=2):
        """
        ä»Spark Web UIè·å–å½“å‰å®éªŒçš„Stageæ•°é‡
        """
        stages_url = f"{spark_master_url}/api/v1/applications/{app_id}/stages"

        for attempt in range(retries):
            try:
                response = requests.get(stages_url, timeout=10)
                if response.status_code == 200:
                    stages_data = response.json()

                    if stages_data:
                        completed_stages = [stage for stage in stages_data
                                          if stage.get('status') == 'COMPLETE']
                        return len(completed_stages)
                    else:
                        return 0
                else:
                    print(f"    âš ï¸  è·å–Stageä¿¡æ¯å¤±è´¥ (HTTP {response.status_code})ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")
            except Exception as e:
                print(f"    âš ï¸  è¿æ¥Spark UIå¤±è´¥: {e}ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")

            if attempt < retries - 1:
                timer.sleep(delay)

        print("    âš ï¸  æ— æ³•è·å–Stageæ•°é‡ï¼Œè¿”å›ä¼°ç®—å€¼")
        return 2  

    def generate_sparse_matrix(self, sc, N, density, partitions):
        """ç”Ÿæˆç¨€ç–çŸ©é˜µå…ƒç´ RDD"""
        print(f"    ğŸ“Š ç”Ÿæˆ {N}x{N} çŸ©é˜µï¼Œå¯†åº¦ {density}")

        # è®¡ç®—éé›¶å…ƒç´ æ•°é‡
        total_elements = N * N
        nnz = int(total_elements * density)

        if nnz == 0:
            return sc.parallelize([], partitions)

        # ç”Ÿæˆéé›¶å…ƒç´ 
        indices = np.random.choice(total_elements, nnz, replace=False)
        rows = indices // N
        cols = indices % N
        values = np.random.rand(nnz)

        # åˆ›å»ºå…ƒç´ åˆ—è¡¨
        elements = list(zip(zip(rows, cols), values))

        return sc.parallelize(elements, partitions)

    def naive_matrix_multiply(self, rdd_A, rdd_B, N):

        # å‡†å¤‡æ•°æ®ï¼šå°†Aå’ŒBéƒ½è½¬æ¢ä¸º((i,j), value)æ ¼å¼
        # A: çŸ©é˜µAçš„å…ƒç´ 
        # B: çŸ©é˜µBçš„å…ƒç´ 

        # æ‰§è¡Œç¬›å¡å°”ç§¯ï¼šå°†Aå’ŒBçš„æ‰€æœ‰å…ƒç´ é…å¯¹
        cartesian_rdd = rdd_A.cartesian(rdd_B)

        # è¿‡æ»¤å’Œè®¡ç®—ï¼šåªè®¡ç®—ç¬¦åˆçŸ©é˜µä¹˜æ³•æ¡ä»¶çš„é…å¯¹
        # çŸ©é˜µä¹˜æ³•æ¡ä»¶ï¼šAçš„åˆ—ç´¢å¼• = Bçš„è¡Œç´¢å¼•
        def filter_and_compute(pair):
            ((i, j), a_val), ((k, l), b_val) = pair
            if j == k:  # Açš„åˆ—ç´¢å¼•ç­‰äºBçš„è¡Œç´¢å¼•
                return (((i, l), a_val * b_val),)
            else:
                return ()

        # æ‰§è¡Œè®¡ç®—
        intermediate_rdd = cartesian_rdd.flatMap(filter_and_compute)

        # èšåˆç›¸åŒä½ç½®çš„ç»“æœ
        result_rdd = intermediate_rdd.reduceByKey(lambda a, b: a + b)

        return result_rdd

    def optimized_baseline_multiply(self, rdd_A, rdd_B, N):

        # å‡†å¤‡Aï¼šæŒ‰åˆ—ç´¢å¼•åˆ†ç»„
        def prepare_A(pair):
            (i, j), val = pair
            return (j, (i, val))  # keyä¸ºåˆ—ç´¢å¼•

        # å‡†å¤‡Bï¼šæŒ‰è¡Œç´¢å¼•åˆ†ç»„
        def prepare_B(pair):
            (i, j), val = pair
            return (i, (j, val))  # keyä¸ºè¡Œç´¢å¼•

        # æ‰§è¡Œjoinæ“ä½œ
        joined_rdd = rdd_A.map(prepare_A).join(rdd_B.map(prepare_B))

        # è®¡ç®—ä¹˜æ³•ç»“æœ
        def compute_product(pair):
            j, ((i, a_val), (k, b_val)) = pair
            # A[i][j] * B[j][k] = C[i][k]
            return ((i, k), a_val * b_val)

        result_rdd = joined_rdd.map(compute_product).reduceByKey(lambda a, b: a + b)      

        return result_rdd

    def run_experiment(self, use_optimized=True):
        all_results = []
        res_conf = self.get_resource_config()

        print(f"{'='*80}")
        print(f"ğŸš€ BaselineçŸ©é˜µä¹˜æ³•æµ‹è¯•ï¼ˆæ— Blockã€æ— Broadcastï¼‰")
        print(f"ğŸ”§ ä½¿ç”¨{'ä¼˜åŒ–ç‰ˆ' if use_optimized else 'æœ´ç´ ç‰ˆ'}ç®—æ³•")
        print(f"ğŸ”§ é…ç½®å‚æ•°: ExecMem={res_conf['executor_memory']}, Partitions={res_conf['partitions']}")
        print(f"{'='*80}")

        try:
            for i, exp in enumerate(self.experiments, 1):
                n = exp['n']
                density = exp['density']
                exp_id = f"Baseline_Exp{i}_{n}x{n}_D{int(density*100)}"

                print(f"\nâ–¶ï¸  [{i}/{len(self.experiments)}] æ­£åœ¨è¿è¡Œ: {n}x{n} | å¯†åº¦ {density}")

                # åˆ›å»ºSpark Session
                conf = (SparkConf()
                    .setAppName(f"Baseline_{exp_id}")
                    .setMaster("spark://172.23.166.133:7077")
                    .set("spark.driver.host", "172.23.166.133")
                    .set("spark.executor.memory", res_conf['executor_memory'])
                    .set("spark.executor.cores", res_conf['executor_cores'])
                    .set("spark.driver.memory", res_conf['driver_memory'])
                    .set("spark.default.parallelism", str(res_conf['partitions']))        
                    .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")                    
                    .set("spark.kryoserializer.buffer.max", "512m")
                    .set("spark.local.dir", "/tmp/spark")
                    .set("spark.ui.showConsoleProgress", "false"))

                spark = SparkSession.builder.config(conf=conf).getOrCreate()
                sc = spark.sparkContext
                sc.setLogLevel("ERROR")

                app_id = sc.applicationId
                print(f"    ğŸ“± Spark App ID: {app_id}")

                try:
                    start_time = time.time()
                    start_cpu = psutil.cpu_percent()
                    start_mem = psutil.virtual_memory().percent

                    # è®¡ç®—çŸ©é˜µå¤§å°
                    matrix_size_mb = (n * n * 8 * density) / (1024 * 1024)
                    print(f"    ğŸ“Š çŸ©é˜µä¼°ç®—å¤§å°: {matrix_size_mb:.2f}MB")

                    if matrix_size_mb > 100 and not use_optimized:
                        print(f"    âš ï¸  è­¦å‘Šï¼šæœ´ç´ ç®—æ³•å¯¹è¾ƒå¤§çŸ©é˜µå¯èƒ½éå¸¸æ…¢ï¼")

                    # 1. ç”Ÿæˆæ•°æ®
                    gen_start = time.time()
                    print(f"    ğŸŒ€ ç”ŸæˆçŸ©é˜µAå’ŒB...")
                    rdd_A = self.generate_sparse_matrix(sc, n, density, res_conf['partitions'])
                    rdd_B = self.generate_sparse_matrix(sc, n, density, res_conf['partitions'])

                    rdd_A.cache().count()
                    rdd_B.cache().count()

                    gen_time = time.time() - gen_start
                    print(f"    â±ï¸  æ•°æ®ç”Ÿæˆè€—æ—¶: {gen_time:.2f}s")

                    calc_start = time.time()

                    # 2. æ‰§è¡ŒçŸ©é˜µä¹˜æ³•
                    print(f"    ğŸ§® å¼€å§‹çŸ©é˜µä¹˜æ³•è®¡ç®—...")
                    if use_optimized:
                        result_rdd = self.optimized_baseline_multiply(rdd_A, rdd_B, n)    
                    else:
                        result_rdd = self.naive_matrix_multiply(rdd_A, rdd_B, n)

                    # è§¦å‘è®¡ç®—
                    print(f"    ğŸ“ˆ è®¡ç®—æœ€ç»ˆç»“æœ...")
                    count = result_rdd.count()

                    calc_end = time.time()
                    compute_time = calc_end - calc_start
                    total_time = calc_end - start_time

                    # 3. è·å–Stage CountæŒ‡æ ‡
                    stage_count = self.get_spark_stage_count(app_id)

                    # 4. è®°å½•æŒ‡æ ‡
                    end_cpu = psutil.cpu_percent()
                    end_mem = psutil.virtual_memory().percent


                    nnz_A = n * n * density
                    nnz_B = n * n * density

                    if use_optimized:
                        # joinæ“ä½œï¼šShuffleä¸¤è¾¹çš„æ•°æ®
                        shuffle_read_mb = (nnz_A + nnz_B) * 8 / (1024 * 1024)
                        shuffle_write_mb = (nnz_A * density + nnz_B * density) * 8 / (1024 * 1024)
                    else:
                        # Cartesian productï¼šShuffleæ‰€æœ‰æ•°æ®
                        shuffle_read_mb = (nnz_A + nnz_B) * 8 / (1024 * 1024)
                        shuffle_write_mb = (nnz_A * nnz_B) * 8 / (1024 * 1024) * density  

                    print(f"\n    âœ… å®éªŒå®Œæˆ!")
                    print(f"    â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s (è®¡ç®—: {compute_time:.2f}s)"
)
                    print(f"    ğŸ“ˆ Stageæ•°: {stage_count} | ç»“æœå…ƒç´ æ•°: {count}")
                    print(f"    ğŸ’¾ Shuffleä¼°ç®—: Read={shuffle_read_mb:.2f}MB, Write={shuffle_write_mb:.2f}MB")
                    print(f"    ğŸ–¥ï¸  CPUåˆ©ç”¨ç‡: {round((start_cpu + end_cpu)/2, 1)}%")     

                    all_results.append({
                        "Matrix Size": n,
                        "Density": density,
                        "Total Time (s)": round(total_time, 2),
                        "Compute Time (s)": round(compute_time, 2),
                        "Gen Time (s)": round(gen_time, 2),
                        "Shuffle Read (MB)": round(shuffle_read_mb, 2),
                        "Shuffle Write (MB)": round(shuffle_write_mb, 2),
                        "Stage Count": stage_count,
                        "CPU (%)": round((start_cpu + end_cpu)/2, 1),
                        "Memory (%)": round((start_mem + end_mem)/2, 1),
                        "Result Elements": count,
                        "Algorithm": "Optimized-Baseline" if use_optimized else "Naive-Baseline",
                        "Status": "Success"
                    })

                    # æ¸…ç†å†…å­˜
                    rdd_A.unpersist()
                    rdd_B.unpersist()

                except Exception as e:
                    print(f"    âŒ å®éªŒå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    all_results.append({
                        "Matrix Size": n,
                        "Density": density,
                        "Total Time (s)": 0,
                        "Compute Time (s)": 0,
                        "Gen Time (s)": 0,
                        "Shuffle Read (MB)": 0.0,
                        "Shuffle Write (MB)": 0.0,
                        "Stage Count": 0,
                        "CPU (%)": 0,
                        "Memory (%)": 0,
                        "Result Elements": 0,
                        "Algorithm": "Optimized-Baseline" if use_optimized else "Naive-Baseline",
                        "Status": f"Failed: {str(e)[:50]}..."
                    })
                finally:
                    spark.stop()
                    print(f"    ğŸ”„ æ¸…ç†Spark Session...")

                timer.sleep(2)

        except Exception as e:
            print(f"âŒ å‘ç”Ÿå…¨å±€é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.save_and_plot(all_results, use_optimized)

    def save_and_plot(self, results, use_optimized):
        df = pd.DataFrame(results)
        algo_type = "optimized" if use_optimized else "naive"
        csv_path = os.path.join(self.results_dir, f"baseline_{algo_type}_metrics.csv")    
        df.to_csv(csv_path, index=False)
        print(f"\nğŸ’¾ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {csv_path}")
        print("\nğŸ“‹ å®éªŒç»“æœæ±‡æ€»:")
        print(df.to_string())

        # è¿‡æ»¤æ‰å¤±è´¥æˆ–è·³è¿‡çš„å®éªŒ
        df_plot = df[df['Status'] == 'Success']

        if len(df_plot) == 0:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸçš„å®éªŒå¯ç»˜å›¾")
            return

        # ç»˜å›¾
        plt.figure(figsize=(12, 8))

        # ä¸åŒå¯†åº¦çš„è€—æ—¶å¯¹æ¯”
        densities = df_plot['Density'].unique()
        x = np.arange(len(densities))

        total_times = []
        compute_times = []
        gen_times = []

        for d in densities:
            data = df_plot[df_plot['Density'] == d]
            if len(data) > 0:
                total_times.append(data['Total Time (s)'].iloc[0])
                compute_times.append(data['Compute Time (s)'].iloc[0])
                gen_times.append(data['Gen Time (s)'].iloc[0])

        # å †å æŸ±çŠ¶å›¾
        plt.bar(x, gen_times, label='æ•°æ®ç”Ÿæˆ', color='lightblue')
        plt.bar(x, compute_times, bottom=gen_times, label='çŸ©é˜µè®¡ç®—', color='lightcoral') 

        plt.xlabel('çŸ©é˜µå¯†åº¦')
        plt.ylabel('æ—¶é—´ (ç§’)')
        plt.title(f'BaselineçŸ©é˜µä¹˜æ³• ({algo_type}ç®—æ³•): 1000x1000çŸ©é˜µ')
        plt.xticks(x, [f'å¯†åº¦ {d}' for d in densities])
        plt.legend()
        plt.grid(axis='y', alpha=0.3)

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i, (total, compute, gen) in enumerate(zip(total_times, compute_times, gen_times)):
            plt.text(i, total + max(total_times)*0.02, f'{total:.1f}s', ha='center')      
            plt.text(i, gen/2, f'{gen:.1f}s', ha='center', color='black')
            plt.text(i, gen + compute/2, f'{compute:.1f}s', ha='center', color='black')   

        plt.tight_layout()

        # ä¿å­˜å›¾ç‰‡
        img_path = os.path.join(self.results_dir, f"baseline_{algo_type}_performance.png")        
        plt.savefig(img_path)
        print(f"ğŸ“Š å›¾è¡¨å·²ç”Ÿæˆ: {img_path}")

        # æ‰“å°åˆ†ææ€»ç»“
        print("\n" + "="*80)
        print(f"ğŸ“ˆ BaselineçŸ©é˜µä¹˜æ³•å®éªŒåˆ†ææ€»ç»“ ({algo_type}ç®—æ³•):")
        print("="*80)

        for _, row in df_plot.iterrows():
            print(f"\nå¯†åº¦ {row['Density']}:")
            print(f"  æ€»è€—æ—¶: {row['Total Time (s)']:.2f}s")
            print(f"  è®¡ç®—æ—¶é—´: {row['Compute Time (s)']:.2f}s")
            print(f"  Shuffle Read: {row['Shuffle Read (MB)']:.2f}MB")
            print(f"  Shuffle Write: {row['Shuffle Write (MB)']:.2f}MB")
            print(f"  ç»“æœå…ƒç´ æ•°: {row['Result Elements']}")

if __name__ == "__main__":
    use_optimized = True

    print("="*80)
    if use_optimized:
        print("è¿è¡Œä¼˜åŒ–ç‰ˆBaselineç®—æ³•ï¼ˆä½¿ç”¨joinæ“ä½œï¼‰")
    else:
        print("è¿è¡Œæœ´ç´ ç‰ˆBaselineç®—æ³•ï¼ˆä½¿ç”¨Cartesian productï¼‰")      
    print("="*80)

    exp = BaselineMatrixExperiment()
    exp.run_experiment(use_optimized=use_optimized)
