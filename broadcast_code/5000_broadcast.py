#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import sys
import os
import time
import psutil
import json
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from datetime import datetime
import requests
import time as timer
from pathlib import Path

SPARK_HOME = Path(os.environ.get("SPARK_HOME", "/usr/local/spark"))
SPARK_MASTER = "spark://172.23.166.133:7077"
DRIVER_HOST = "172.23.166.133"
PYTHON_PATH = "/usr/bin/python3"
SPARK_UI_URL = "http://172.23.166.133:4040"

sys.path.insert(0, os.path.join(SPARK_HOME, "python"))
sys.path.insert(0, os.path.join(SPARK_HOME, "python", "lib", "py4j-0.10.9.7-src.zip"))
sys.path.insert(0, os.path.join(SPARK_HOME, "python", "lib", "pyspark.zip"))

from pyspark.sql import SparkSession
from pyspark import SparkConf

class Broadcast5000Experiment:
    def __init__(self):
        self.results_dir = "broadcast_5000_results"
        os.makedirs(self.results_dir, exist_ok=True)

        self.experiments = [
            {"n": 5000, "density": 0.01,  "block_size": 5000},
            {"n": 5000, "density": 0.1, "block_size": 5000},
        ]

    def get_resource_config(self):
        return {
            "executor_memory": "2g",
            "executor_cores": "1",
            "driver_memory": "2g",
            "partitions": 12
        }

    def get_spark_stage_count(self, app_id, retries=5, delay=2):
        stages_url = f"{SPARK_UI_URL}/api/v1/applications/{app_id}/stages"

        for attempt in range(retries):
            try:
                response = requests.get(stages_url, timeout=10)
                if response.status_code == 200:
                    stages_data = response.json()

                    if stages_data:
                        completed_stages = [stage for stage in stages_data
                                          if stage.get('status') == 'COMPLETE']
                        return len(completed_stages)
                    else:
                        return 0
                else:
                    print(f"    âš ï¸  è·å–Stageä¿¡æ¯å¤±è´¥ (HTTP {response.status_code})ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")
            except Exception as e:
                print(f"    âš ï¸  è¿æ¥Spark UIå¤±è´¥: {e}ï¼Œç¬¬{attempt+1}æ¬¡é‡è¯•...")

            if attempt < retries - 1:
                timer.sleep(delay)

        print("    âš ï¸  æ— æ³•è·å–Stageæ•°é‡ï¼Œè¿”å›ä¼°ç®—å€¼")
        return 3

    def generate_sparse_matrix_optimized(self, sc, N, density, partitions):
        print(f"    ğŸ“Š ç”Ÿæˆ {N}x{N} ç¨€ç–çŸ©é˜µï¼Œå¯†åº¦ {density}")

        total_elements = N * N
        nnz = int(total_elements * density)

        if nnz == 0:
            return sc.parallelize([], partitions)

        indices = np.random.choice(total_elements, nnz, replace=False)

        rows = indices // N
        cols = indices % N

        values = np.random.rand(nnz)

        elements = list(zip(zip(rows, cols), values))

        batch_size = 100000
        rdd_list = []

        for i in range(0, len(elements), batch_size):
            batch = elements[i:i+batch_size]
            rdd_batch = sc.parallelize(batch, min(partitions, len(batch)))
            rdd_list.append(rdd_batch)

        if len(rdd_list) == 1:
            return rdd_list[0]
        else:
            return sc.union(rdd_list)

    def optimized_broadcast_multiply(self, sc, rdd_A, rdd_B, n):
        print(f"    ğŸ”„ ä¼˜åŒ–ç‰ˆå¹¿æ’­çŸ©é˜µä¹˜æ³•...")

        print(f"    ğŸ“¡ æ”¶é›†BçŸ©é˜µåˆ°Driver...")
        B_rows_local = rdd_B.collect()
        print(f"    ï¿½ï¿½ BçŸ©é˜µæ”¶é›†å®Œæˆ: {len(B_rows_local)}ä¸ªéé›¶å…ƒç´ ")

        B_by_rows = {}
        for (i, j), val in B_rows_local:
            if i not in B_by_rows:
                B_by_rows[i] = {}
            B_by_rows[i][j] = val

        bc_B = sc.broadcast(B_by_rows)
        print(f"    ğŸ“¡ å¹¿æ’­BçŸ©é˜µå®Œæˆ: {len(B_by_rows)}è¡Œ")

        print(f"    ğŸ“Š æŒ‰è¡Œåˆ†ç»„AçŸ©é˜µ...")
        def prepare_A_rows(pair):
            (i, j), val = pair
            return (i, (j, val))

        A_by_rows = rdd_A.map(prepare_A_rows).groupByKey()
        print(f"    ğŸ“Š AçŸ©é˜µæŒ‰è¡Œåˆ†ç»„å®Œæˆ")

        def multiply_rows(row_pair):
            i, a_cols_iter = row_pair
            b_dict = bc_B.value

            a_row = dict(list(a_cols_iter))

            results = []
            for k, b_row in b_dict.items():
                dot_product = 0
                for j, a_val in a_row.items():
                    if j in b_row:
                        dot_product += a_val * b_row[j]

                if abs(dot_product) > 1e-10:
                    results.append(((i, k), dot_product))

            return results

        print(f"    âš™ï¸  å¼€å§‹çŸ©é˜µä¹˜æ³•è®¡ç®—...")
        result_rdd = A_by_rows.flatMap(multiply_rows)

        return result_rdd, bc_B

    def run_experiment(self):
        all_results = []
        res_conf = self.get_resource_config()

        print(f"{'='*80}")
        print(f"ğŸš€ Broadcast-Only 5000x5000 æµ‹è¯•")
        print(f"ğŸ”§ é…ç½®å‚æ•°: ExecMem={res_conf['executor_memory']}, Partitions={res_conf['partitions']}")
        print(f"{'='*80}")

        try:
            for i, exp in enumerate(self.experiments, 1):
                n = exp['n']
                density = exp['density']
                block_size = exp['block_size']
                exp_id = f"Broadcast5000_Exp{i}_{n}x{n}_D{int(density*100)}"

                print(f"\nâ–¶ï¸  [{i}/{len(self.experiments)}] æ­£åœ¨è¿è¡Œ: {n}x{n} | å¯†åº¦ {density}")

                conf = (SparkConf()
                    .setAppName(f"Broadcast5000_{exp_id}")
                    .setMaster(SPARK_MASTER)
                    .set("spark.driver.host", DRIVER_HOST)
                    .set("spark.executor.instances", "3")
                    .set("spark.executor.memory", res_conf['executor_memory'])
                    .set("spark.executor.cores", res_conf['executor_cores'])
                    .set("spark.driver.memory", "4g")
                    .set("spark.default.parallelism", str(res_conf['partitions']))
                    .set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
                    .set("spark.kryoserializer.buffer.max", "512m")
                    .set("spark.local.dir", "/tmp/spark")
                    .set("spark.ui.showConsoleProgress", "false")
                    .set("spark.driver.maxResultSize", "2g")
                    .set("spark.memory.fraction", "0.8")
                    .set("spark.memory.storageFraction", "0.3")
                    .set("spark.shuffle.service.enabled", "true")
                    .set("spark.python.executable", PYTHON_PATH))

                spark = SparkSession.builder.config(conf=conf).getOrCreate()
                sc = spark.sparkContext
                sc.setLogLevel("ERROR")

                app_id = sc.applicationId
                print(f"    ğŸ“± Spark App ID: {app_id}")

                try:
                    start_time = time.time()
                    start_cpu = psutil.cpu_percent()
                    start_mem = psutil.virtual_memory().percent

                    matrix_size_mb = (n * n * 8 * density) / (1024 * 1024)
                    print(f"    ğŸ“Š çŸ©é˜µä¼°ç®—å¤§å°: {matrix_size_mb:.2f}MB")

                    if matrix_size_mb > 500:
                        print(f"    âš ï¸  è­¦å‘Šï¼šçŸ©é˜µè¾ƒå¤§ ({matrix_size_mb:.1f}MB)ï¼Œå¹¿æ’­å¯èƒ½å¯¼è‡´Driverå†…å­˜ä¸è¶³")
                        if density >= 0.1:
                            print(f"    âš ï¸  è€ƒè™‘è·³è¿‡å¯†åº¦ {density} çš„å®éªŒ")

                    gen_start = time.time()
                    print(f"    ğŸŒ€ ç”ŸæˆçŸ©é˜µA...")
                    rdd_A = self.generate_sparse_matrix_optimized(sc, n, density, res_conf['partitions'])
                    print(f"    ğŸŒ€ ç”ŸæˆçŸ©é˜µB...")
                    rdd_B = self.generate_sparse_matrix_optimized(sc, n, density, res_conf['partitions'])

                    print(f"    ğŸ’¾ ç¼“å­˜çŸ©é˜µ...")
                    rdd_A.cache().count()
                    rdd_B.cache().count()

                    gen_time = time.time() - gen_start
                    print(f"    â±ï¸  æ•°æ®ç”Ÿæˆè€—æ—¶: {gen_time:.2f}s")

                    calc_start = time.time()

                    print(f"    ğŸ§® å¼€å§‹çŸ©é˜µä¹˜æ³•è®¡ç®—...")
                    result_rdd, bc_B = self.optimized_broadcast_multiply(sc, rdd_A, rdd_B, n)

                    print(f"    ğŸ“ˆ è®¡ç®—æœ€ç»ˆç»“æœ...")
                    count = result_rdd.count()

                    calc_end = time.time()
                    compute_time = calc_end - calc_start
                    total_time = calc_end - start_time

                    stage_count = self.get_spark_stage_count(app_id)

                    end_cpu = psutil.cpu_percent()
                    end_mem = psutil.virtual_memory().percent

                    total_elements = n * n * density
                    shuffle_write_mb = (total_elements * 8) / (1024 * 1024)

                    print(f"\n    âœ… å®éªŒå®Œæˆ!")
                    print(f"    â±ï¸  æ€»è€—æ—¶: {total_time:.2f}s (è®¡ç®—: {compute_time:.2f}s)")
                    print(f"    ğŸ“ˆ Stageæ•°: {stage_count} | ç»“æœå…ƒç´ æ•°: {count}")
                    print(f"    ğŸ’¾ Shuffle Writeä¼°ç®—: {shuffle_write_mb:.2f}MB")
                    print(f"    ğŸ–¥ï¸  CPUåˆ©ç”¨ç‡: {round((start_cpu + end_cpu)/2, 1)}%")
                    print(f"    ğŸ’½ å†…å­˜åˆ©ç”¨ç‡: {round((start_mem + end_mem)/2, 1)}%")

                    all_results.append({
                        "Matrix Size": n,
                        "Density": density,
                        "Total Time (s)": round(total_time, 2),
                        "Compute Time (s)": round(compute_time, 2),
                        "Gen Time (s)": round(gen_time, 2),
                        "Shuffle Read (MB)": 0.0,
                        "Shuffle Write (MB)": round(shuffle_write_mb, 2),
                        "Stage Count": stage_count,
                        "CPU (%)": round((start_cpu + end_cpu)/2, 1),
                        "Memory (%)": round((start_mem + end_mem)/2, 1),
                        "Result Elements": count,
                        "Algorithm": "Optimized-Broadcast",
                        "Status": "Success"
                    })

                    bc_B.unpersist()
                    rdd_A.unpersist()
                    rdd_B.unpersist()

                except Exception as e:
                    print(f"    âŒ å®éªŒå¤±è´¥: {e}")
                    import traceback
                    traceback.print_exc()
                    all_results.append({
                        "Matrix Size": n,
                        "Density": density,
                        "Total Time (s)": 0,
                        "Compute Time (s)": 0,
                        "Gen Time (s)": 0,
                        "Shuffle Read (MB)": 0.0,
                        "Shuffle Write (MB)": 0.0,
                        "Stage Count": 0,
                        "CPU (%)": 0,
                        "Memory (%)": 0,
                        "Result Elements": 0,
                        "Algorithm": "Optimized-Broadcast",
                        "Status": f"Failed: {str(e)[:50]}..."
                    })
                finally:
                    spark.stop()
                    print(f"    ğŸ”„ æ¸…ç†Spark Session...")

                timer.sleep(3)

        except Exception as e:
            print(f"âŒ å‘ç”Ÿå…¨å±€é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            self.save_and_plot(all_results)

    def save_and_plot(self, results):
        df = pd.DataFrame(results)
        csv_path = os.path.join(self.results_dir, "broadcast_5000_metrics.csv")
        df.to_csv(csv_path, index=False)
        print(f"\nğŸ’¾ è¯¦ç»†æ•°æ®å·²ä¿å­˜: {csv_path}")
        print("\nğŸ“‹ å®éªŒç»“æœæ±‡æ€»:")
        print(df.to_string())

        df_plot = df[df['Status'] == 'Success']

        if len(df_plot) == 0:
            print("âš ï¸  æ²¡æœ‰æˆåŠŸçš„å®éªŒå¯ç»˜å›¾")
            return

        plt.figure(figsize=(12, 8))

        densities = df_plot['Density'].unique()
        x = np.arange(len(densities))

        total_times = []
        compute_times = []
        gen_times = []

        for d in densities:
            data = df_plot[df_plot['Density'] == d]
            if len(data) > 0:
                total_times.append(data['Total Time (s)'].iloc[0])
                compute_times.append(data['Compute Time (s)'].iloc[0])
                gen_times.append(data['Gen Time (s)'].iloc[0])

        plt.bar(x, gen_times, label='æ•°æ®ç”Ÿæˆ', color='lightblue')
        plt.bar(x, compute_times, bottom=gen_times, label='çŸ©é˜µè®¡ç®—', color='lightgreen')

        plt.xlabel('çŸ©é˜µå¯†åº¦')
        plt.ylabel('æ—¶é—´ (ç§’)')
        plt.title('Broadcast-Only 5000x5000 çŸ©é˜µæ€§èƒ½åˆ†æ')
        plt.xticks(x, [f'å¯†åº¦ {d}' for d in densities])
        plt.legend()
        plt.grid(axis='y', alpha=0.3)

        for i, (total, compute, gen) in enumerate(zip(total_times, compute_times, gen_times)):
            plt.text(i, total + max(total_times)*0.02, f'{total:.1f}s', ha='center')
            plt.text(i, gen/2, f'{gen:.1f}s', ha='center', color='black')
            plt.text(i, gen + compute/2, f'{compute:.1f}s', ha='center', color='black')

        plt.tight_layout()

        img_path = os.path.join(self.results_dir, "broadcast_5000_performance.png")
        plt.savefig(img_path)
        print(f"ğŸ“Š å›¾è¡¨å·²ç”Ÿæˆ: {img_path}")

        print("\n" + "="*80)
        print("ğŸ” Broadcast-Only 5000x5000 å®éªŒåˆ†ææ€»ç»“:")
        print("="*80)

        for _, row in df_plot.iterrows():
            print(f"\nå¯†åº¦ {row['Density']}:")
            print(f"  æ€»è€—æ—¶: {row['Total Time (s)']:.2f}s")
            print(f"  è®¡ç®—æ—¶é—´: {row['Compute Time (s)']:.2f}s")
            print(f"  æ•°æ®ç”Ÿæˆ: {row['Gen Time (s)']:.2f}s")
            print(f"  Shuffle Read: {row['Shuffle Read (MB)']:.2f}MB")
            print(f"  ç»“æœå…ƒç´ æ•°: {row['Result Elements']}")

        print("\nğŸ” å…³é”®è§‚å¯Ÿ:")
        print("1. Shuffle Readå§‹ç»ˆä¸º0 - å¹¿æ’­ä¼˜åŒ–å®Œå…¨æœ‰æ•ˆ")
        print("2. æ•°æ®ç”Ÿæˆæ—¶é—´å¯èƒ½å ä¸»å¯¼ - ç¨€ç–çŸ©é˜µç”Ÿæˆæœ‰å¼€é”€")
        print("3. ç¨ å¯†çŸ©é˜µ(å¯†åº¦1.0)å¯èƒ½éœ€è¦æ›´å¤šå†…å­˜")

if __name__ == "__main__":
    print("="*80)
    print("ğŸš€ å¼€å§‹è¿è¡Œ Broadcast-Only 5000x5000 æµ‹è¯•")
    print("âš ï¸  æ³¨æ„ï¼šç¨ å¯†çŸ©é˜µå¯èƒ½éœ€è¦è¾ƒå¤šå†…å­˜")
    print("="*80)

    exp = Broadcast5000Experiment()
    exp.run_experiment()
